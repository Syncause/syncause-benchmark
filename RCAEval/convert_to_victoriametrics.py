"""
å°† RE2-OB metrics è½¬æ¢ä¸º VictoriaMetrics æ ¼å¼
æ”¯æŒå¤šç§å¯¼å‡ºæ ¼å¼ï¼š
1. Prometheus Remote Write (JSON)
2. InfluxDB Line Protocol
3. VictoriaMetrics Import (JSON Lines)
"""
import os
import json
import pandas as pd
import argparse
from pathlib import Path
from datetime import datetime
from typing import Dict, List


def load_k8s_info(case_dir: str) -> Dict:
    """åŠ è½½ K8s é›†ç¾¤ä¿¡æ¯"""
    case_path = Path(case_dir)
    
    # è¯»å– pod-node æ˜ å°„
    pod_node_files = list(case_path.glob('pod-node-*.csv'))
    pod_to_node = {}
    
    if pod_node_files:
        df = pd.read_csv(pod_node_files[0])
        for _, row in df.iterrows():
            pod_name = row['POD']
            node_name = row['NODE_NAME']
            service_name = '-'.join(pod_name.split('-')[:-2])
            pod_to_node[service_name] = {
                'pod': pod_name,
                'node': node_name
            }
    
    # è¯»å– cluster info
    cluster_info = {}
    cluster_info_file = case_path / 'cluster_info.json'
    if cluster_info_file.exists():
        with open(cluster_info_file, 'r') as f:
            cluster_info = json.load(f)
    
    return {
        'pod_to_node': pod_to_node,
        'cluster_info': cluster_info
    }


def convert_to_influxdb_line_protocol(
    metrics_df: pd.DataFrame,
    k8s_info: Dict,
    case_name: str,
    output_file: str
):
    """
    è½¬æ¢ä¸º InfluxDB Line Protocol æ ¼å¼
    æ ¼å¼: measurement,tag1=value1,tag2=value2 field1=value1,field2=value2 timestamp
    """
    print(f"\nğŸ”„ è½¬æ¢ä¸º InfluxDB Line Protocol æ ¼å¼...")
    
    lines = []
    metric_cols = [col for col in metrics_df.columns if col != 'time']
    
    for idx, row in metrics_df.iterrows():
        timestamp_ns = int(row['time'] * 1e9)  # è½¬æ¢ä¸ºçº³ç§’
        
        for metric_col in metric_cols:
            value = row[metric_col]
            
            # è·³è¿‡ NaN å€¼
            if pd.isna(value):
                continue
            
            # è§£æ metric åç§°å’ŒæœåŠ¡
            parts = metric_col.split('_')
            if len(parts) < 2:
                continue
            
            service = '_'.join(parts[:-1])
            metric_type = parts[-1]
            
            # æ„å»ºæ ‡ç­¾
            tags = [
                f'case="{case_name}"',
                f'service="{service}"',
                f'metric_type="{metric_type}"'
            ]
            
            # æ·»åŠ  K8s ä¿¡æ¯
            if service in k8s_info.get('pod_to_node', {}):
                k8s_data = k8s_info['pod_to_node'][service]
                tags.append(f'pod="{k8s_data["pod"]}"')
                tags.append(f'node="{k8s_data["node"]}"')
            
            # æ„å»º line protocol
            # measurement,tag1=val1,tag2=val2 field=value timestamp
            line = f"{metric_type},{','.join(tags)} value={value} {timestamp_ns}"
            lines.append(line)
        
        if (idx + 1) % 100 == 0:
            print(f"  å¤„ç†è¿›åº¦: {idx + 1}/{len(metrics_df)} è¡Œ")
    
    # å†™å…¥æ–‡ä»¶
    with open(output_file, 'w') as f:
        f.write('\n'.join(lines))
    
    print(f"âœ… InfluxDB Line Protocol æ ¼å¼å·²ä¿å­˜: {output_file}")
    print(f"   æ€»è¡Œæ•°: {len(lines)}")


def convert_to_victoriametrics_json(
    metrics_df: pd.DataFrame,
    k8s_info: Dict,
    case_name: str,
    output_file: str
):
    """
    è½¬æ¢ä¸º VictoriaMetrics Import æ ¼å¼ (JSON Lines)
    æ¯è¡Œæ˜¯ä¸€ä¸ª JSON å¯¹è±¡
    """
    print(f"\nğŸ”„ è½¬æ¢ä¸º VictoriaMetrics JSON æ ¼å¼...")
    
    lines = []
    metric_cols = [col for col in metrics_df.columns if col != 'time']
    
    for idx, row in metrics_df.iterrows():
        timestamp_ms = int(row['time'] * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
        
        for metric_col in metric_cols:
            value = row[metric_col]
            
            # è·³è¿‡ NaN å€¼
            if pd.isna(value):
                continue
            
            # è§£æ metric åç§°å’ŒæœåŠ¡
            parts = metric_col.split('_')
            if len(parts) < 2:
                continue
            
            service = '_'.join(parts[:-1])
            metric_type = parts[-1]
            
            # æ„å»º labels
            labels = {
                'case': case_name,
                'service': service,
                'metric_type': metric_type,
                '__name__': metric_type  # VictoriaMetrics è¦æ±‚
            }
            
            # æ·»åŠ  K8s ä¿¡æ¯
            if service in k8s_info.get('pod_to_node', {}):
                k8s_data = k8s_info['pod_to_node'][service]
                labels['pod'] = k8s_data['pod']
                labels['node'] = k8s_data['node']
            
            # æ„å»º JSON å¯¹è±¡
            metric_obj = {
                'metric': labels,
                'values': [value],
                'timestamps': [timestamp_ms]
            }
            
            lines.append(json.dumps(metric_obj))
        
        if (idx + 1) % 100 == 0:
            print(f"  å¤„ç†è¿›åº¦: {idx + 1}/{len(metrics_df)} è¡Œ")
    
    # å†™å…¥æ–‡ä»¶
    with open(output_file, 'w') as f:
        f.write('\n'.join(lines))
    
    print(f"âœ… VictoriaMetrics JSON æ ¼å¼å·²ä¿å­˜: {output_file}")
    print(f"   æ€»è¡Œæ•°: {len(lines)}")


def convert_to_prometheus_json(
    metrics_df: pd.DataFrame,
    k8s_info: Dict,
    case_name: str,
    output_file: str
):
    """
    è½¬æ¢ä¸º Prometheus Remote Write æ ¼å¼ (JSON)
    è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„ JSON å¯¹è±¡ï¼ŒåŒ…å«æ‰€æœ‰æ—¶é—´åºåˆ—
    """
    print(f"\nğŸ”„ è½¬æ¢ä¸º Prometheus Remote Write æ ¼å¼...")
    
    timeseries_map = {}  # key: metric_signature, value: {labels, samples}
    metric_cols = [col for col in metrics_df.columns if col != 'time']
    
    for idx, row in metrics_df.iterrows():
        timestamp_ms = int(row['time'] * 1000)
        
        for metric_col in metric_cols:
            value = row[metric_col]
            
            if pd.isna(value):
                continue
            
            # è§£æ metric
            parts = metric_col.split('_')
            if len(parts) < 2:
                continue
            
            service = '_'.join(parts[:-1])
            metric_type = parts[-1]
            
            # æ„å»º metric ç­¾åï¼ˆç”¨äºå»é‡ï¼‰
            metric_name = f"{service}_{metric_type}"
            
            # æ„å»ºæ ‡ç­¾
            labels = [
                {'name': '__name__', 'value': metric_name},
                {'name': 'case', 'value': case_name},
                {'name': 'service', 'value': service},
                {'name': 'metric_type', 'value': metric_type}
            ]
            
            # æ·»åŠ  K8s ä¿¡æ¯
            if service in k8s_info.get('pod_to_node', {}):
                k8s_data = k8s_info['pod_to_node'][service]
                labels.append({'name': 'pod', 'value': k8s_data['pod']})
                labels.append({'name': 'node', 'value': k8s_data['node']})
            
            # åˆ›å»ºç­¾å
            label_sig = tuple(sorted([(l['name'], l['value']) for l in labels]))
            
            # æ·»åŠ æ ·æœ¬
            if label_sig not in timeseries_map:
                timeseries_map[label_sig] = {
                    'labels': labels,
                    'samples': []
                }
            
            timeseries_map[label_sig]['samples'].append({
                'value': value,
                'timestamp': timestamp_ms
            })
        
        if (idx + 1) % 100 == 0:
            print(f"  å¤„ç†è¿›åº¦: {idx + 1}/{len(metrics_df)} è¡Œ")
    
    # æ„å»ºæœ€ç»ˆçš„ JSON ç»“æ„
    timeseries_list = []
    for ts_data in timeseries_map.values():
        timeseries_list.append({
            'labels': ts_data['labels'],
            'samples': ts_data['samples']
        })
    
    prometheus_data = {
        'timeseries': timeseries_list
    }
    
    # å†™å…¥æ–‡ä»¶
    with open(output_file, 'w') as f:
        json.dump(prometheus_data, f, indent=2)
    
    print(f"âœ… Prometheus JSON æ ¼å¼å·²ä¿å­˜: {output_file}")
    print(f"   æ—¶é—´åºåˆ—æ•°: {len(timeseries_list)}")


def convert_case(
    case_dir: str,
    output_dir: str,
    formats: List[str] = ['influxdb', 'vm-json', 'prometheus']
):
    """
    è½¬æ¢å•ä¸ªæ¡ˆä¾‹
    
    Args:
        case_dir: æ¡ˆä¾‹ç›®å½•
        output_dir: è¾“å‡ºç›®å½•
        formats: è¦ç”Ÿæˆçš„æ ¼å¼åˆ—è¡¨
    """
    case_path = Path(case_dir)
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    print(f"\n{'='*70}")
    print(f"ğŸ“‚ å¤„ç†æ¡ˆä¾‹: {case_path}")
    print(f"{'='*70}")
    
    # è¯»å–æ•°æ®
    metrics_file = case_path / 'simple_metrics.csv'
    if not metrics_file.exists():
        print(f"âš ï¸  Metrics æ–‡ä»¶ä¸å­˜åœ¨: {metrics_file}")
        return
    
    print(f"ğŸ“Š è¯»å– metrics æ•°æ®...")
    metrics_df = pd.read_csv(metrics_file)
    print(f"   æ•°æ®å½¢çŠ¶: {metrics_df.shape}")
    
    # è¯»å– K8s ä¿¡æ¯
    print(f"ğŸ”— è¯»å– K8s ä¿¡æ¯...")
    k8s_info = load_k8s_info(str(case_path))
    print(f"   æœåŠ¡æ•°: {len(k8s_info.get('pod_to_node', {}))}")
    
    # è·å–æ¡ˆä¾‹åç§°
    case_name = case_path.parent.name + '_' + case_path.name
    
    # è½¬æ¢ä¸ºå„ç§æ ¼å¼
    if 'influxdb' in formats:
        influxdb_file = output_path / 'metrics.influxdb'
        convert_to_influxdb_line_protocol(
            metrics_df, k8s_info, case_name, str(influxdb_file)
        )
    
    if 'vm-json' in formats:
        vm_json_file = output_path / 'metrics.vm.jsonl'
        convert_to_victoriametrics_json(
            metrics_df, k8s_info, case_name, str(vm_json_file)
        )
    
    if 'prometheus' in formats:
        prom_file = output_path / 'metrics.prometheus.json'
        convert_to_prometheus_json(
            metrics_df, k8s_info, case_name, str(prom_file)
        )
    
    # å¤åˆ¶å…¶ä»–å…ƒæ•°æ®
    print(f"\nğŸ“‹ å¤åˆ¶å…ƒæ•°æ®æ–‡ä»¶...")
    import shutil
    for filename in ['inject_time.txt', 'cluster_info.json']:
        src = case_path / filename
        if src.exists():
            dst = output_path / filename
            shutil.copy2(src, dst)
            print(f"   âœ“ {filename}")
    
    print(f"\nâœ… è½¬æ¢å®Œæˆ! è¾“å‡ºç›®å½•: {output_path}")


def convert_dataset(
    dataset_dir: str,
    output_base_dir: str,
    formats: List[str],
    limit: int = None
):
    """è½¬æ¢æ•´ä¸ªæ•°æ®é›†"""
    dataset_path = Path(dataset_dir)
    
    print(f"\n{'='*70}")
    print(f"ğŸš€ å¼€å§‹è½¬æ¢æ•°æ®é›†åˆ° VictoriaMetrics æ ¼å¼")
    print(f"{'='*70}")
    print(f"è¾“å…¥: {dataset_dir}")
    print(f"è¾“å‡º: {output_base_dir}")
    print(f"æ ¼å¼: {', '.join(formats)}")
    if limit:
        print(f"é™åˆ¶: ä»…è½¬æ¢å‰ {limit} ä¸ªæ¡ˆä¾‹")
    print()
    
    # éå†æ‰€æœ‰æ¡ˆä¾‹
    fault_dirs = [d for d in dataset_path.iterdir() if d.is_dir() and not d.name.startswith('.')]
    
    total_cases = 0
    for fault_dir in sorted(fault_dirs):
        case_dirs = [d for d in fault_dir.iterdir() if d.is_dir() and d.name.isdigit()]
        
        for case_dir in sorted(case_dirs, key=lambda x: int(x.name)):
            if limit and total_cases >= limit:
                break
            
            relative_path = case_dir.relative_to(dataset_path)
            output_dir = Path(output_base_dir) / relative_path
            
            convert_case(str(case_dir), str(output_dir), formats)
            total_cases += 1
        
        if limit and total_cases >= limit:
            break
    
    print(f"\n{'='*70}")
    print(f"âœ… æ•°æ®é›†è½¬æ¢å®Œæˆ!")
    print(f"{'='*70}")
    print(f"æ€»æ¡ˆä¾‹æ•°: {total_cases}")
    print()


def main():
    parser = argparse.ArgumentParser(
        description='å°† RE2-OB metrics è½¬æ¢ä¸º VictoriaMetrics æ ¼å¼'
    )
    parser.add_argument(
        '--input',
        type=str,
        default='data/RE2/RE2-OB',
        help='è¾“å…¥æ•°æ®é›†ç›®å½•'
    )
    parser.add_argument(
        '--output',
        type=str,
        default='data/RE2/RE2-OB-VictoriaMetrics',
        help='è¾“å‡ºç›®å½•'
    )
    parser.add_argument(
        '--case',
        type=str,
        help='åªè½¬æ¢æŒ‡å®šæ¡ˆä¾‹ (ä¾‹å¦‚: checkoutservice_cpu/1)'
    )
    parser.add_argument(
        '--format',
        type=str,
        nargs='+',
        choices=['influxdb', 'vm-json', 'prometheus', 'all'],
        default=['all'],
        help='è¾“å‡ºæ ¼å¼ (å¯é€‰å¤šä¸ª)'
    )
    parser.add_argument(
        '--limit',
        type=int,
        help='é™åˆ¶è½¬æ¢çš„æ¡ˆä¾‹æ•°é‡'
    )
    
    args = parser.parse_args()
    
    # å¤„ç†æ ¼å¼å‚æ•°
    if 'all' in args.format:
        formats = ['influxdb', 'vm-json', 'prometheus']
    else:
        formats = args.format
    
    if args.case:
        # è½¬æ¢å•ä¸ªæ¡ˆä¾‹
        case_dir = os.path.join(args.input, args.case)
        output_dir = os.path.join(args.output, args.case)
        convert_case(case_dir, output_dir, formats)
    else:
        # è½¬æ¢æ•´ä¸ªæ•°æ®é›†
        convert_dataset(args.input, args.output, formats, args.limit)


if __name__ == '__main__':
    main()


