"""
Â∞Ü RE2-OB metrics ËΩ¨Êç¢‰∏∫ VictoriaMetrics Ê†ºÂºè
ÊîØÊåÅÂ§öÁßçÂØºÂá∫Ê†ºÂºèÔºö
1. Prometheus Remote Write (JSON)
2. InfluxDB Line Protocol
3. VictoriaMetrics Import (JSON Lines)
"""
import os
import json
import pandas as pd
import argparse
from pathlib import Path
from datetime import datetime
from typing import Dict, List


def load_k8s_info(case_dir: str) -> Dict:
    """Âä†ËΩΩ K8s ÈõÜÁæ§‰ø°ÊÅØ"""
    case_path = Path(case_dir)
    
    # ËØªÂèñ pod-node Êò†Â∞Ñ
    pod_node_files = list(case_path.glob('pod-node-*.csv'))
    pod_to_node = {}
    
    if pod_node_files:
        df = pd.read_csv(pod_node_files[0])
        for _, row in df.iterrows():
            pod_name = row['POD']
            node_name = row['NODE_NAME']
            service_name = '-'.join(pod_name.split('-')[:-2])
            pod_to_node[service_name] = {
                'pod': pod_name,
                'node': node_name
            }
    
    # ËØªÂèñ cluster info
    cluster_info = {}
    cluster_info_file = case_path / 'cluster_info.json'
    if cluster_info_file.exists():
        with open(cluster_info_file, 'r') as f:
            cluster_info = json.load(f)
    
    return {
        'pod_to_node': pod_to_node,
        'cluster_info': cluster_info
    }


def convert_to_influxdb_line_protocol(
    metrics_df: pd.DataFrame,
    k8s_info: Dict,
    case_name: str,
    output_file: str
):
    """
    ËΩ¨Êç¢‰∏∫ InfluxDB Line Protocol Ê†ºÂºè
    Ê†ºÂºè: measurement,tag1=value1,tag2=value2 field1=value1,field2=value2 timestamp
    """
    print(f"\nüîÑ ËΩ¨Êç¢‰∏∫ InfluxDB Line Protocol Ê†ºÂºè...")
    
    lines = []
    metric_cols = [col for col in metrics_df.columns if col != 'time']
    
    for idx, row in metrics_df.iterrows():
        timestamp_ns = int(row['time'] * 1e9)  # ËΩ¨Êç¢‰∏∫Á∫≥Áßí
        
        for metric_col in metric_cols:
            value = row[metric_col]
            
            # Ë∑≥Ëøá NaN ÂÄº
            if pd.isna(value):
                continue
            
            # Ëß£Êûê metric ÂêçÁß∞ÂíåÊúçÂä°
            parts = metric_col.split('_')
            if len(parts) < 2:
                continue
            
            service = '_'.join(parts[:-1])
            metric_type = parts[-1]
            
            # ÊûÑÂª∫Ê†áÁ≠æ
            tags = [
                f'case="{case_name}"',
                f'service="{service}"',
                f'metric_type="{metric_type}"'
            ]
            
            # Ê∑ªÂä† K8s ‰ø°ÊÅØ
            if service in k8s_info.get('pod_to_node', {}):
                k8s_data = k8s_info['pod_to_node'][service]
                tags.append(f'pod="{k8s_data["pod"]}"')
                tags.append(f'node="{k8s_data["node"]}"')
            
            # ÊûÑÂª∫ line protocol
            # measurement,tag1=val1,tag2=val2 field=value timestamp
            line = f"{metric_type},{','.join(tags)} value={value} {timestamp_ns}"
            lines.append(line)
        
        if (idx + 1) % 100 == 0:
            print(f"  Â§ÑÁêÜËøõÂ∫¶: {idx + 1}/{len(metrics_df)} Ë°å")
    
    # ÂÜôÂÖ•Êñá‰ª∂
    with open(output_file, 'w') as f:
        f.write('\n'.join(lines))
    
    print(f"‚úÖ InfluxDB Line Protocol Ê†ºÂºèÂ∑≤‰øùÂ≠ò: {output_file}")
    print(f"   ÊÄªË°åÊï∞: {len(lines)}")


def convert_to_victoriametrics_json(
    metrics_df: pd.DataFrame,
    k8s_info: Dict,
    case_name: str,
    output_file: str
):
    """
    ËΩ¨Êç¢‰∏∫ VictoriaMetrics Import Ê†ºÂºè (JSON Lines)
    ÊØèË°åÊòØ‰∏Ä‰∏™ JSON ÂØπË±°
    """
    print(f"\nüîÑ ËΩ¨Êç¢‰∏∫ VictoriaMetrics JSON Ê†ºÂºè...")
    
    lines = []
    metric_cols = [col for col in metrics_df.columns if col != 'time']
    
    for idx, row in metrics_df.iterrows():
        timestamp_ms = int(row['time'] * 1000)  # ËΩ¨Êç¢‰∏∫ÊØ´Áßí
        
        for metric_col in metric_cols:
            value = row[metric_col]
            
            # Ë∑≥Ëøá NaN ÂÄº
            if pd.isna(value):
                continue
            
            # Ëß£Êûê metric ÂêçÁß∞ÂíåÊúçÂä°
            parts = metric_col.split('_')
            if len(parts) < 2:
                continue
            
            service = '_'.join(parts[:-1])
            metric_type = parts[-1]
            
            # ÊûÑÂª∫ labels
            labels = {
                'case': case_name,
                'service': service,
                'metric_type': metric_type,
                '__name__': metric_type  # VictoriaMetrics Ë¶ÅÊ±Ç
            }
            
            # Ê∑ªÂä† K8s ‰ø°ÊÅØ
            if service in k8s_info.get('pod_to_node', {}):
                k8s_data = k8s_info['pod_to_node'][service]
                labels['pod'] = k8s_data['pod']
                labels['node'] = k8s_data['node']
            
            # ÊûÑÂª∫ JSON ÂØπË±°
            metric_obj = {
                'metric': labels,
                'values': [value],
                'timestamps': [timestamp_ms]
            }
            
            lines.append(json.dumps(metric_obj))
        
        if (idx + 1) % 100 == 0:
            print(f"  Â§ÑÁêÜËøõÂ∫¶: {idx + 1}/{len(metrics_df)} Ë°å")
    
    # ÂÜôÂÖ•Êñá‰ª∂
    with open(output_file, 'w') as f:
        f.write('\n'.join(lines))
    
    print(f"‚úÖ VictoriaMetrics JSON Ê†ºÂºèÂ∑≤‰øùÂ≠ò: {output_file}")
    print(f"   ÊÄªË°åÊï∞: {len(lines)}")


def convert_to_prometheus_json(
    metrics_df: pd.DataFrame,
    k8s_info: Dict,
    case_name: str,
    output_file: str
):
    """
    ËΩ¨Êç¢‰∏∫ Prometheus Remote Write Ê†ºÂºè (JSON)
    ËøôÊòØ‰∏Ä‰∏™ÂÆåÊï¥ÁöÑ JSON ÂØπË±°ÔºåÂåÖÂê´ÊâÄÊúâÊó∂Èó¥Â∫èÂàó
    """
    print(f"\nüîÑ ËΩ¨Êç¢‰∏∫ Prometheus Remote Write Ê†ºÂºè...")
    
    timeseries_map = {}  # key: metric_signature, value: {labels, samples}
    metric_cols = [col for col in metrics_df.columns if col != 'time']
    
    for idx, row in metrics_df.iterrows():
        timestamp_ms = int(row['time'] * 1000)
        
        for metric_col in metric_cols:
            value = row[metric_col]
            
            if pd.isna(value):
                continue
            
            # Ëß£Êûê metric
            parts = metric_col.split('_')
            if len(parts) < 2:
                continue
            
            service = '_'.join(parts[:-1])
            metric_type = parts[-1]
            
            # ÊûÑÂª∫ metric Á≠æÂêçÔºàÁî®‰∫éÂéªÈáçÔºâ
            metric_name = f"{service}_{metric_type}"
            
            # ÊûÑÂª∫Ê†áÁ≠æ
            labels = [
                {'name': '__name__', 'value': metric_name},
                {'name': 'case', 'value': case_name},
                {'name': 'service', 'value': service},
                {'name': 'metric_type', 'value': metric_type}
            ]
            
            # Ê∑ªÂä† K8s ‰ø°ÊÅØ
            if service in k8s_info.get('pod_to_node', {}):
                k8s_data = k8s_info['pod_to_node'][service]
                labels.append({'name': 'pod', 'value': k8s_data['pod']})
                labels.append({'name': 'node', 'value': k8s_data['node']})
            
            # ÂàõÂª∫Á≠æÂêç
            label_sig = tuple(sorted([(l['name'], l['value']) for l in labels]))
            
            # Ê∑ªÂä†Ê†∑Êú¨
            if label_sig not in timeseries_map:
                timeseries_map[label_sig] = {
                    'labels': labels,
                    'samples': []
                }
            
            timeseries_map[label_sig]['samples'].append({
                'value': value,
                'timestamp': timestamp_ms
            })
        
        if (idx + 1) % 100 == 0:
            print(f"  Â§ÑÁêÜËøõÂ∫¶: {idx + 1}/{len(metrics_df)} Ë°å")
    
    # ÊûÑÂª∫ÊúÄÁªàÁöÑ JSON ÁªìÊûÑ
    timeseries_list = []
    for ts_data in timeseries_map.values():
        timeseries_list.append({
            'labels': ts_data['labels'],
            'samples': ts_data['samples']
        })
    
    prometheus_data = {
        'timeseries': timeseries_list
    }
    
    # ÂÜôÂÖ•Êñá‰ª∂
    with open(output_file, 'w') as f:
        json.dump(prometheus_data, f, indent=2)
    
    print(f"‚úÖ Prometheus JSON Ê†ºÂºèÂ∑≤‰øùÂ≠ò: {output_file}")
    print(f"   Êó∂Èó¥Â∫èÂàóÊï∞: {len(timeseries_list)}")


def convert_case(
    case_dir: str,
    output_dir: str,
    formats: List[str] = ['influxdb', 'vm-json', 'prometheus']
):
    """
    ËΩ¨Êç¢Âçï‰∏™Ê°à‰æã
    
    Args:
        case_dir: Ê°à‰æãÁõÆÂΩï
        output_dir: ËæìÂá∫ÁõÆÂΩï
        formats: Ë¶ÅÁîüÊàêÁöÑÊ†ºÂºèÂàóË°®
    """
    case_path = Path(case_dir)
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    print(f"\n{'='*70}")
    print(f"üìÇ Â§ÑÁêÜÊ°à‰æã: {case_path}")
    print(f"{'='*70}")
    
    # ËØªÂèñÊï∞ÊçÆ
    metrics_file = case_path / 'simple_metrics.csv'
    if not metrics_file.exists():
        print(f"‚ö†Ô∏è  Metrics Êñá‰ª∂‰∏çÂ≠òÂú®: {metrics_file}")
        return
    
    print(f"üìä ËØªÂèñ metrics Êï∞ÊçÆ...")
    metrics_df = pd.read_csv(metrics_file)
    print(f"   Êï∞ÊçÆÂΩ¢Áä∂: {metrics_df.shape}")
    
    # ËØªÂèñ K8s ‰ø°ÊÅØ
    print(f"üîó ËØªÂèñ K8s ‰ø°ÊÅØ...")
    k8s_info = load_k8s_info(str(case_path))
    print(f"   ÊúçÂä°Êï∞: {len(k8s_info.get('pod_to_node', {}))}")
    
    # Ëé∑ÂèñÊ°à‰æãÂêçÁß∞
    case_name = case_path.parent.name + '_' + case_path.name
    
    # ËΩ¨Êç¢‰∏∫ÂêÑÁßçÊ†ºÂºè
    if 'influxdb' in formats:
        influxdb_file = output_path / 'metrics.influxdb'
        convert_to_influxdb_line_protocol(
            metrics_df, k8s_info, case_name, str(influxdb_file)
        )
    
    if 'vm-json' in formats:
        vm_json_file = output_path / 'metrics.vm.jsonl'
        convert_to_victoriametrics_json(
            metrics_df, k8s_info, case_name, str(vm_json_file)
        )
    
    if 'prometheus' in formats:
        prom_file = output_path / 'metrics.prometheus.json'
        convert_to_prometheus_json(
            metrics_df, k8s_info, case_name, str(prom_file)
        )
    
    # Â§çÂà∂ÂÖ∂‰ªñÂÖÉÊï∞ÊçÆ
    print(f"\nüìã Â§çÂà∂ÂÖÉÊï∞ÊçÆÊñá‰ª∂...")
    import shutil
    for filename in ['inject_time.txt', 'cluster_info.json']:
        src = case_path / filename
        if src.exists():
            dst = output_path / filename
            shutil.copy2(src, dst)
            print(f"   ‚úì {filename}")
    
    print(f"\n‚úÖ ËΩ¨Êç¢ÂÆåÊàê! ËæìÂá∫ÁõÆÂΩï: {output_path}")


def convert_dataset(
    dataset_dir: str,
    output_base_dir: str,
    formats: List[str],
    limit: int = None
):
    """ËΩ¨Êç¢Êï¥‰∏™Êï∞ÊçÆÈõÜ"""
    dataset_path = Path(dataset_dir)
    
    print(f"\n{'='*70}")
    print(f"üöÄ ÂºÄÂßãËΩ¨Êç¢Êï∞ÊçÆÈõÜÂà∞ VictoriaMetrics Ê†ºÂºè")
    print(f"{'='*70}")
    print(f"ËæìÂÖ•: {dataset_dir}")
    print(f"ËæìÂá∫: {output_base_dir}")
    print(f"Ê†ºÂºè: {', '.join(formats)}")
    if limit:
        print(f"ÈôêÂà∂: ‰ªÖËΩ¨Êç¢Ââç {limit} ‰∏™Ê°à‰æã")
    print()
    
    # ÈÅçÂéÜÊâÄÊúâÊ°à‰æã
    fault_dirs = [d for d in dataset_path.iterdir() if d.is_dir() and not d.name.startswith('.')]
    
    total_cases = 0
    for fault_dir in sorted(fault_dirs):
        case_dirs = [d for d in fault_dir.iterdir() if d.is_dir() and d.name.isdigit()]
        
        for case_dir in sorted(case_dirs, key=lambda x: int(x.name)):
            if limit and total_cases >= limit:
                break
            
            relative_path = case_dir.relative_to(dataset_path)
            output_dir = Path(output_base_dir) / relative_path
            
            convert_case(str(case_dir), str(output_dir), formats)
            total_cases += 1
        
        if limit and total_cases >= limit:
            break
    
    print(f"\n{'='*70}")
    print(f"‚úÖ Êï∞ÊçÆÈõÜËΩ¨Êç¢ÂÆåÊàê!")
    print(f"{'='*70}")
    print(f"ÊÄªÊ°à‰æãÊï∞: {total_cases}")
    print()


def main():
    parser = argparse.ArgumentParser(
        description='Â∞Ü RE2-OB metrics ËΩ¨Êç¢‰∏∫ VictoriaMetrics Ê†ºÂºè'
    )
    parser.add_argument(
        '--input',
        type=str,
        default='data/RE2/RE2-OB',
        help='ËæìÂÖ•Êï∞ÊçÆÈõÜÁõÆÂΩï'
    )
    parser.add_argument(
        '--output',
        type=str,
        default='data/RE2/RE2-OB-VictoriaMetrics',
        help='ËæìÂá∫ÁõÆÂΩï'
    )
    parser.add_argument(
        '--case',
        type=str,
        help='Âè™ËΩ¨Êç¢ÊåáÂÆöÊ°à‰æã (‰æãÂ¶Ç: checkoutservice_cpu/1)'
    )
    parser.add_argument(
        '--format',
        type=str,
        nargs='+',
        choices=['influxdb', 'vm-json', 'prometheus', 'all'],
        default=['all'],
        help='ËæìÂá∫Ê†ºÂºè (ÂèØÈÄâÂ§ö‰∏™)'
    )
    parser.add_argument(
        '--limit',
        type=int,
        help='ÈôêÂà∂ËΩ¨Êç¢ÁöÑÊ°à‰æãÊï∞Èáè'
    )
    
    args = parser.parse_args()
    
    # Â§ÑÁêÜÊ†ºÂºèÂèÇÊï∞
    if 'all' in args.format:
        formats = ['influxdb', 'vm-json', 'prometheus']
    else:
        formats = args.format
    
    if args.case:
        # ËΩ¨Êç¢Âçï‰∏™Ê°à‰æã
        case_dir = os.path.join(args.input, args.case)
        output_dir = os.path.join(args.output, args.case)
        convert_case(case_dir, output_dir, formats)
    else:
        # ËΩ¨Êç¢Êï¥‰∏™Êï∞ÊçÆÈõÜ
        convert_dataset(args.input, args.output, formats, args.limit)


if __name__ == '__main__':
    main()


