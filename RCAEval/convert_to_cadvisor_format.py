"""
å°† RE2-OB metrics è½¬æ¢ä¸º cAdvisor Prometheus æ ¼å¼
æ¨¡æ‹Ÿ cAdvisor çš„æŒ‡æ ‡å‘½åå’Œæ ‡ç­¾ç»“æ„
"""
import os
import json
import pandas as pd
import argparse
from pathlib import Path
from typing import Dict, List


def load_k8s_info(case_dir: str) -> Dict:
    """åŠ è½½ K8s é›†ç¾¤ä¿¡æ¯"""
    case_path = Path(case_dir)
    
    # è¯»å– pod-node æ˜ å°„
    pod_node_files = list(case_path.glob('pod-node-*.csv'))
    pod_to_node = {}
    
    if pod_node_files:
        df = pd.read_csv(pod_node_files[0])
        for _, row in df.iterrows():
            pod_name = row['POD']
            node_name = row['NODE_NAME']
            service_name = '-'.join(pod_name.split('-')[:-2])
            pod_to_node[service_name] = {
                'pod': pod_name,
                'pod_name': pod_name,
                'node': node_name,
                'namespace': 'default',  # RE2-OB ä½¿ç”¨é»˜è®¤å‘½åç©ºé—´
                'container': service_name,  # å®¹å™¨åé€šå¸¸å’ŒæœåŠ¡åä¸€è‡´
            }
    
    # è¯»å– cluster info
    cluster_info = {}
    cluster_info_file = case_path / 'cluster_info.json'
    if cluster_info_file.exists():
        with open(cluster_info_file, 'r') as f:
            cluster_info = json.load(f)
    
    return {
        'pod_to_node': pod_to_node,
        'cluster_info': cluster_info
    }


def convert_to_cadvisor_prometheus(
    metrics_df: pd.DataFrame,
    k8s_info: Dict,
    case_name: str,
    output_file: str
):
    """
    è½¬æ¢ä¸º cAdvisor Prometheus æ ¼å¼
    
    cAdvisor æŒ‡æ ‡æ ¼å¼:
    - container_cpu_usage_seconds_total
    - container_memory_working_set_bytes
    - container_network_receive_bytes_total
    - container_fs_io_time_seconds_total
    
    æ ‡ç­¾:
    - namespace
    - pod
    - container
    - name (å®¹å™¨åç§°)
    - image
    """
    print(f"\nğŸ”„ è½¬æ¢ä¸º cAdvisor Prometheus æ ¼å¼...")
    
    lines = []
    metric_cols = [col for col in metrics_df.columns if col != 'time']
    
    # cAdvisor æŒ‡æ ‡æ˜ å°„
    metric_mapping = {
        'cpu': {
            'name': 'container_cpu_usage_seconds_total',
            'type': 'counter',
            'help': 'Cumulative cpu time consumed in seconds',
            'unit': 'seconds',
            'is_cumulative': True
        },
        'mem': {
            'name': 'container_memory_working_set_bytes',
            'type': 'gauge',
            'help': 'Current working set of the container in bytes',
            'unit': 'bytes',
            'is_cumulative': False
        },
        'diskio': {
            'name': 'container_fs_io_time_seconds_total',
            'type': 'counter',
            'help': 'Cumulative count of seconds spent doing I/Os',
            'unit': 'seconds',
            'is_cumulative': True
        },
        'socket': {
            'name': 'container_sockets',
            'type': 'gauge',
            'help': 'Number of open sockets',
            'unit': 'sockets',
            'is_cumulative': False
        },
        'workload': {
            'name': 'container_requests_total',
            'type': 'counter',
            'help': 'Total number of requests',
            'unit': 'requests',
            'is_cumulative': True
        },
        'latency-50': {
            'name': 'container_request_duration_seconds',
            'type': 'gauge',
            'help': 'Request duration in seconds (p50)',
            'unit': 'seconds',
            'is_cumulative': False,
            'quantile': '0.5'
        },
        'latency-90': {
            'name': 'container_request_duration_seconds',
            'type': 'gauge',
            'help': 'Request duration in seconds (p90)',
            'unit': 'seconds',
            'is_cumulative': False,
            'quantile': '0.9'
        },
        'error': {
            'name': 'container_request_errors_total',
            'type': 'counter',
            'help': 'Total number of request errors',
            'unit': 'errors',
            'is_cumulative': True
        }
    }
    
    # å†™å…¥ HELP å’Œ TYPE æ³¨é‡Š
    help_written = set()
    
    # ç´¯ç§¯å€¼è·Ÿè¸ªï¼ˆç”¨äº counter ç±»å‹ï¼‰
    cumulative_values = {}
    
    for idx, row in metrics_df.iterrows():
        timestamp_sec = int(row["time"])  # è½¬æ¢ä¸ºæ¯«ç§’
        
        for metric_col in metric_cols:
            value = row[metric_col]
            
            # è·³è¿‡ NaN å€¼
            if pd.isna(value):
                continue
            
            # è§£æ metric åç§°å’ŒæœåŠ¡
            parts = metric_col.split('_')
            if len(parts) < 2:
                continue
            
            service = '_'.join(parts[:-1])
            metric_type = parts[-1]
            
            # è·å–æ˜ å°„çš„ cAdvisor æŒ‡æ ‡å
            if metric_type not in metric_mapping:
                continue
            
            metric_info = metric_mapping[metric_type]
            cadvisor_metric_name = metric_info['name']
            
            # å†™å…¥ HELP å’Œ TYPEï¼ˆæ¯ä¸ªæŒ‡æ ‡åªå†™ä¸€æ¬¡ï¼‰
            if cadvisor_metric_name not in help_written:
                lines.append(f"# HELP {cadvisor_metric_name} {metric_info['help']}")
                lines.append(f"# TYPE {cadvisor_metric_name} {metric_info['type']}")
                help_written.add(cadvisor_metric_name)
            
            # æ„å»ºæ ‡ç­¾
            labels = [
                f'namespace="default"',
                f'container="{service}"',
                f'name="{service}"',
            ]
            
            # æ·»åŠ  K8s ä¿¡æ¯
            if service in k8s_info.get('pod_to_node', {}):
                k8s_data = k8s_info['pod_to_node'][service]
                labels.append(f'pod="{k8s_data["pod"]}"')
                labels.append(f'node="{k8s_data["node"]}"')
                labels.append(f'image="{service}:latest"')  # å‡è®¾é•œåƒå
            
            # æ·»åŠ åˆ†ä½æ•°æ ‡ç­¾ï¼ˆå¦‚æœæ˜¯å»¶è¿ŸæŒ‡æ ‡ï¼‰
            if 'quantile' in metric_info:
                labels.append(f'quantile="{metric_info["quantile"]}"')
            
            # å¤„ç†ç´¯ç§¯å€¼ï¼ˆcounter ç±»å‹ï¼‰
            if metric_info['is_cumulative']:
                key = f"{cadvisor_metric_name}_{service}"
                if key not in cumulative_values:
                    cumulative_values[key] = 0
                
                # CPU: ç™¾åˆ†æ¯”è½¬æ¢ä¸ºç´¯ç§¯ç§’æ•°
                if metric_type == 'cpu':
                    # å‡è®¾æ¯ä¸ªé‡‡æ ·é—´éš”æ˜¯ 1 ç§’ï¼ŒCPU ç™¾åˆ†æ¯”ç´¯åŠ 
                    cumulative_values[key] += value / 100  # è½¬æ¢ä¸ºç§’
                    final_value = cumulative_values[key]
                # å…¶ä»–ç´¯ç§¯æŒ‡æ ‡ç›´æ¥ç´¯åŠ 
                else:
                    cumulative_values[key] += value
                    final_value = cumulative_values[key]
            else:
                # Gauge ç±»å‹ç›´æ¥ä½¿ç”¨å½“å‰å€¼
                if metric_type == 'mem':
                    final_value = value  # å†…å­˜å·²ç»æ˜¯å­—èŠ‚
                elif 'latency' in metric_type:
                    final_value = value  # å»¶è¿Ÿå·²ç»æ˜¯ç§’
                else:
                    final_value = value
            
            # æ„å»º Prometheus æ ¼å¼
            # metric_name{label1="value1",label2="value2"} value timestamp
            label_str = ','.join(labels)
            line = f"{cadvisor_metric_name}{{{label_str}}} {final_value} {timestamp_sec}"
            lines.append(line)
        
        if (idx + 1) % 100 == 0:
            print(f"  å¤„ç†è¿›åº¦: {idx + 1}/{len(metrics_df)} è¡Œ")
    
    # å†™å…¥æ–‡ä»¶
    with open(output_file, 'w') as f:
        f.write('\n'.join(lines))
    
    print(f"âœ… cAdvisor Prometheus æ ¼å¼å·²ä¿å­˜: {output_file}")
    print(f"   æ€»è¡Œæ•°: {len(lines)}")
    print(f"   æŒ‡æ ‡ç±»å‹: {len(help_written)}")


def convert_to_cadvisor_json(
    metrics_df: pd.DataFrame,
    k8s_info: Dict,
    case_name: str,
    output_file: str
):
    """
    è½¬æ¢ä¸º cAdvisor JSON æ ¼å¼ï¼ˆç”¨äº remote writeï¼‰
    """
    print(f"\nğŸ”„ è½¬æ¢ä¸º cAdvisor JSON æ ¼å¼...")
    
    lines = []
    metric_cols = [col for col in metrics_df.columns if col != 'time']
    
    # æŒ‡æ ‡æ˜ å°„ï¼ˆåŒä¸Šï¼‰
    metric_mapping = {
        'cpu': 'container_cpu_usage_seconds_total',
        'mem': 'container_memory_working_set_bytes',
        'diskio': 'container_fs_io_time_seconds_total',
        'socket': 'container_sockets',
        'workload': 'container_requests_total',
        'latency-50': 'container_request_duration_seconds',
        'latency-90': 'container_request_duration_seconds',
        'error': 'container_request_errors_total'
    }
    
    cumulative_values = {}
    
    for idx, row in metrics_df.iterrows():
        timestamp_sec = int(row["time"])
        
        for metric_col in metric_cols:
            value = row[metric_col]
            
            if pd.isna(value):
                continue
            
            parts = metric_col.split('_')
            if len(parts) < 2:
                continue
            
            service = '_'.join(parts[:-1])
            metric_type = parts[-1]
            
            if metric_type not in metric_mapping:
                continue
            
            cadvisor_metric_name = metric_mapping[metric_type]
            
            # æ„å»º labels
            labels = {
                '__name__': cadvisor_metric_name,
                'namespace': 'default',
                'container': service,
                'name': service
            }
            
            # æ·»åŠ  K8s ä¿¡æ¯
            if service in k8s_info.get('pod_to_node', {}):
                k8s_data = k8s_info['pod_to_node'][service]
                labels['pod'] = k8s_data['pod']
                labels['node'] = k8s_data['node']
                labels['image'] = f"{service}:latest"
            
            # æ·»åŠ åˆ†ä½æ•°
            if 'latency' in metric_type:
                labels['quantile'] = '0.5' if '50' in metric_type else '0.9'
            
            # å¤„ç†ç´¯ç§¯å€¼
            if metric_type in ['cpu', 'diskio', 'workload', 'error']:
                key = f"{cadvisor_metric_name}_{service}"
                if key not in cumulative_values:
                    cumulative_values[key] = 0
                
                if metric_type == 'cpu':
                    cumulative_values[key] += value / 100
                else:
                    cumulative_values[key] += value
                final_value = cumulative_values[key]
            else:
                final_value = value
            
            # æ„å»º JSON å¯¹è±¡
            metric_obj = {
                'metric': labels,
                'values': [final_value],
                'timestamps': [timestamp_sec]
            }
            
            lines.append(json.dumps(metric_obj))
        
        if (idx + 1) % 100 == 0:
            print(f"  å¤„ç†è¿›åº¦: {idx + 1}/{len(metrics_df)} è¡Œ")
    
    # å†™å…¥æ–‡ä»¶
    with open(output_file, 'w') as f:
        f.write('\n'.join(lines))
    
    print(f"âœ… cAdvisor JSON æ ¼å¼å·²ä¿å­˜: {output_file}")
    print(f"   æ€»è¡Œæ•°: {len(lines)}")


def convert_case(
    case_dir: str,
    output_dir: str,
    formats: List[str] = ['prometheus', 'json']
):
    """è½¬æ¢å•ä¸ªæ¡ˆä¾‹"""
    case_path = Path(case_dir)
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    print(f"\n{'='*70}")
    print(f"ğŸ“‚ å¤„ç†æ¡ˆä¾‹: {case_path}")
    print(f"{'='*70}")
    
    # è¯»å–æ•°æ®
    metrics_file = case_path / 'simple_metrics.csv'
    if not metrics_file.exists():
        print(f"âš ï¸  Metrics æ–‡ä»¶ä¸å­˜åœ¨: {metrics_file}")
        return
    
    print(f"ğŸ“Š è¯»å– metrics æ•°æ®...")
    metrics_df = pd.read_csv(metrics_file)
    print(f"   æ•°æ®å½¢çŠ¶: {metrics_df.shape}")
    
    # è¯»å– K8s ä¿¡æ¯
    print(f"ğŸ”— è¯»å– K8s ä¿¡æ¯...")
    k8s_info = load_k8s_info(str(case_path))
    print(f"   æœåŠ¡æ•°: {len(k8s_info.get('pod_to_node', {}))}")
    
    # è·å–æ¡ˆä¾‹åç§°
    case_name = case_path.parent.name + '_' + case_path.name
    
    # è½¬æ¢ä¸ºå„ç§æ ¼å¼
    if 'prometheus' in formats:
        prom_file = output_path / 'metrics.cadvisor.prom'
        convert_to_cadvisor_prometheus(
            metrics_df, k8s_info, case_name, str(prom_file)
        )
    
    if 'json' in formats:
        json_file = output_path / 'metrics.cadvisor.jsonl'
        convert_to_cadvisor_json(
            metrics_df, k8s_info, case_name, str(json_file)
        )
    
    # å¤åˆ¶å…¶ä»–å…ƒæ•°æ®
    print(f"\nğŸ“‹ å¤åˆ¶å…ƒæ•°æ®æ–‡ä»¶...")
    import shutil
    for filename in ['inject_time.txt', 'cluster_info.json']:
        src = case_path / filename
        if src.exists():
            dst = output_path / filename
            shutil.copy2(src, dst)
            print(f"   âœ“ {filename}")
    
    # ä¿å­˜ K8s æ˜ å°„ä¿¡æ¯
    with open(output_path / 'k8s_mapping.json', 'w') as f:
        json.dump(k8s_info['pod_to_node'], f, indent=2)
    print(f"   âœ“ k8s_mapping.json")
    
    print(f"\nâœ… è½¬æ¢å®Œæˆ! è¾“å‡ºç›®å½•: {output_path}")


def main():
    parser = argparse.ArgumentParser(
        description='å°† RE2-OB metrics è½¬æ¢ä¸º cAdvisor Prometheus æ ¼å¼'
    )
    parser.add_argument(
        '--input',
        type=str,
        default='data/RE2/RE2-OB',
        help='è¾“å…¥æ•°æ®é›†ç›®å½•'
    )
    parser.add_argument(
        '--output',
        type=str,
        default='data/RE2/RE2-OB-cAdvisor',
        help='è¾“å‡ºç›®å½•'
    )
    parser.add_argument(
        '--case',
        type=str,
        help='åªè½¬æ¢æŒ‡å®šæ¡ˆä¾‹ (ä¾‹å¦‚: checkoutservice_cpu/1)'
    )
    parser.add_argument(
        '--format',
        type=str,
        nargs='+',
        choices=['prometheus', 'json', 'all'],
        default=['all'],
        help='è¾“å‡ºæ ¼å¼'
    )
    parser.add_argument(
        '--limit',
        type=int,
        help='é™åˆ¶è½¬æ¢çš„æ¡ˆä¾‹æ•°é‡'
    )
    
    args = parser.parse_args()
    
    # å¤„ç†æ ¼å¼å‚æ•°
    if 'all' in args.format:
        formats = ['prometheus', 'json']
    else:
        formats = args.format
    
    if args.case:
        # è½¬æ¢å•ä¸ªæ¡ˆä¾‹
        case_dir = os.path.join(args.input, args.case)
        output_dir = os.path.join(args.output, args.case)
        convert_case(case_dir, output_dir, formats)
    else:
        # è½¬æ¢æ•´ä¸ªæ•°æ®é›†
        import glob
        from tqdm import tqdm
        
        print("="*70)
        print("ğŸš€ å¼€å§‹è½¬æ¢æ•°æ®é›†åˆ° cAdvisor æ ¼å¼")
        print("="*70)
        print(f"è¾“å…¥: {args.input}")
        print(f"è¾“å‡º: {args.output}")
        print(f"æ ¼å¼: {', '.join(formats)}")
        if args.limit:
            print(f"é™åˆ¶: ä»…è½¬æ¢å‰ {args.limit} ä¸ªæ¡ˆä¾‹")
        print()
        
        base_dir = Path(args.input)
        case_dirs = sorted(glob.glob(os.path.join(args.input, '*/*/')))
        
        if args.limit:
            case_dirs = case_dirs[:args.limit]
        
        for case_dir in tqdm(case_dirs, desc="Processing cases"):
            relative_path = Path(case_dir).relative_to(base_dir)
            output_dir = Path(args.output) / relative_path
            convert_case(case_dir, str(output_dir), formats)
        
        print(f"\n{'='*70}")
        print(f"âœ… æ•°æ®é›†è½¬æ¢å®Œæˆ!")
        print(f"{'='*70}")
        print(f"æ€»æ¡ˆä¾‹æ•°: {len(case_dirs)}")
        print()


if __name__ == '__main__':
    main()





